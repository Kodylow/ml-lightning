{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('df_paths.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the indexed breakdown of each element in the 'hops' tuples:\n",
    "\n",
    "0. df_hop_sorted['hop_index']: This is the index of the hop in the path. It starts from 0 and increments by 1 for each subsequent hop in the path.\n",
    "\n",
    "1. df_hop_sorted['pubkey']: This is the public key of the node that the hop is going to. It uniquely identifies a node in the network.\n",
    "\n",
    "2. df_hop_sorted['source_pubkey']: This is the public key of the node where the hop is coming from. It also uniquely identifies a node in the network.\n",
    "\n",
    "3. df_hop_sorted['is_final_hop']: This is a boolean value indicating whether the hop is the final hop in the path. If it's True, this means the hop is going to the final destination node.\n",
    "\n",
    "4. df_hop_sorted['scid']: This is the short channel ID (scid) of the channel used for the hop. It uniquely identifies a channel in the network.\n",
    "\n",
    "5. df_hop_sorted['failure']: This is a boolean value indicating whether the hop failed. If it's True, this means the payment failed at this hop.\n",
    "\n",
    "So, each tuple in the 'hops' list represents a hop in the path, and the elements of the tuple provide information about the hop. The index of each element in the tuple is as listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0. 2. 0. 0. 1. 0.]\\n [1. 1. 2. 1. 0. 1.]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(2.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0. 3. 0. 0. 2. 0.]\\n [1. 4. 3. 0. 3. 1.]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0. 2. 0. 0. 1. 0.]\\n [1. 6. 2. 0. 6. 1.]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0. 3. 0. 0. 2. 0.]\\n [1. 7. 3. 0. 8. 1.]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[ 0.  3.  0.  0.  2.  0.]\\n [ 1.  8.  3.  0. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops  path_failure  \\\n",
       "0        [[0. 2. 0. 0. 1. 0.]\\n [1. 1. 2. 1. 0. 1.]]           1.0   \n",
       "1        [[0. 3. 0. 0. 2. 0.]\\n [1. 4. 3. 0. 3. 1.]]           1.0   \n",
       "2        [[0. 2. 0. 0. 1. 0.]\\n [1. 6. 2. 0. 6. 1.]]           1.0   \n",
       "3        [[0. 3. 0. 0. 2. 0.]\\n [1. 7. 3. 0. 8. 1.]]           1.0   \n",
       "4  [[ 0.  3.  0.  0.  2.  0.]\\n [ 1.  8.  3.  0. ...           1.0   \n",
       "\n",
       "         path_amount duration_seconds  \n",
       "0  tensor(50000000.)       tensor(2.)  \n",
       "1  tensor(50000000.)       tensor(1.)  \n",
       "2  tensor(50000000.)       tensor(1.)  \n",
       "3  tensor(50000000.)       tensor(1.)  \n",
       "4  tensor(50000000.)       tensor(1.)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hops                 object\n",
       "path_failure        float64\n",
       "path_amount          object\n",
       "duration_seconds     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 2....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(2.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 4.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 6.0, 2....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 7.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 8.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops  path_failure  \\\n",
       "0  [[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 2....           1.0   \n",
       "1  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 4.0, 3....           1.0   \n",
       "2  [[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 6.0, 2....           1.0   \n",
       "3  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 7.0, 3....           1.0   \n",
       "4  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 8.0, 3....           1.0   \n",
       "\n",
       "         path_amount duration_seconds  \n",
       "0  tensor(50000000.)       tensor(2.)  \n",
       "1  tensor(50000000.)       tensor(1.)  \n",
       "2  tensor(50000000.)       tensor(1.)  \n",
       "3  tensor(50000000.)       tensor(1.)  \n",
       "4  tensor(50000000.)       tensor(1.)  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def str_to_2dlist(s):\n",
    "    s = re.sub(r'\\s+', ', ', s.replace('\\n', '').replace('[ ', '[').replace(' ]', ']'))  # Remove newline characters and extra spaces\n",
    "    s = re.sub(r'\\[,', '[', s)  # Remove leading commas in each sub-list\n",
    "    return eval(s)\n",
    "\n",
    "# Convert 'hops' column to 2D numpy arrays\n",
    "data['hops'] = data['hops'].apply(lambda x: np.array(str_to_2dlist(x)))\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0], [1.0, 1.0, 2.0, 1....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(2.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 4.0, 3.0, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0], [1.0, 6.0, 2.0, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 7.0, 3.0, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 8.0, 3.0, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops  path_failure  \\\n",
       "0  [[0.0, 2.0, 0.0, 0.0, 1.0], [1.0, 1.0, 2.0, 1....           1.0   \n",
       "1  [[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 4.0, 3.0, 0....           1.0   \n",
       "2  [[0.0, 2.0, 0.0, 0.0, 1.0], [1.0, 6.0, 2.0, 0....           1.0   \n",
       "3  [[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 7.0, 3.0, 0....           1.0   \n",
       "4  [[0.0, 3.0, 0.0, 0.0, 2.0], [1.0, 8.0, 3.0, 0....           1.0   \n",
       "\n",
       "         path_amount duration_seconds  \n",
       "0  tensor(50000000.)       tensor(2.)  \n",
       "1  tensor(50000000.)       tensor(1.)  \n",
       "2  tensor(50000000.)       tensor(1.)  \n",
       "3  tensor(50000000.)       tensor(1.)  \n",
       "4  tensor(50000000.)       tensor(1.)  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hops'] = data['hops'].apply(lambda x: [i[:-1] for i in x])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 2....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 4.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 6.0, 2....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 7.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 8.0, 3....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops  path_failure  \\\n",
       "0  [[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 2....           1.0   \n",
       "1  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 4.0, 3....           1.0   \n",
       "2  [[0.0, 2.0, 0.0, 0.0, 1.0, 0.0], [1.0, 6.0, 2....           1.0   \n",
       "3  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 7.0, 3....           1.0   \n",
       "4  [[0.0, 3.0, 0.0, 0.0, 2.0, 0.0], [1.0, 8.0, 3....           1.0   \n",
       "\n",
       "   path_amount  duration_seconds  \n",
       "0   50000000.0               2.0  \n",
       "1   50000000.0               1.0  \n",
       "2   50000000.0               1.0  \n",
       "3   50000000.0               1.0  \n",
       "4   50000000.0               1.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def str_to_float(s):\n",
    "    # Remove the 'tensor' part from the string\n",
    "    s = s.replace('tensor(', '').replace(')', '')\n",
    "    # Convert the string to a float\n",
    "    return float(s)\n",
    "\n",
    "data['path_amount'] = data['path_amount'].apply(str_to_float)\n",
    "data['duration_seconds'] = data['duration_seconds'].apply(str_to_float)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 'hops' column into a 1D list\n",
    "data['hops'] = data['hops'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "\n",
    "# Create separate columns for each element in the 'hops' list\n",
    "for i in range(data['hops'].apply(len).max()):\n",
    "    data[f'hops_{i}'] = data['hops'].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "\n",
    "# Drop the original 'hops' column\n",
    "data = data.drop('hops', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('duration_seconds', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Split data into features and target\n",
    "target = data['path_failure']\n",
    "features = data.drop('path_failure', axis=1)\n",
    "features = features.fillna(0)\n",
    "\n",
    "# Split data into train and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "features_train, target_train = smote.fit_resample(features_train, target_train)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "features_train, features_val, target_train, target_val = train_test_split(features_train, target_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    582031\n",
      "0.0    581841\n",
      "Name: path_failure, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the 'path_failure' variable in the training set\n",
    "print(target_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new directory to save the data\n",
    "os.makedirs('cleaned_data', exist_ok=True)\n",
    "\n",
    "# Save the data\n",
    "pickle.dump(features_train, open('cleaned_data/features_train.pkl', 'wb'))\n",
    "pickle.dump(target_train, open('cleaned_data/target_train.pkl', 'wb'))\n",
    "pickle.dump(features_test, open('cleaned_data/features_test.pkl', 'wb'))\n",
    "pickle.dump(target_test, open('cleaned_data/target_test.pkl', 'wb'))\n",
    "pickle.dump(features_val, open('cleaned_data/features_val.pkl', 'wb'))\n",
    "pickle.dump(target_val, open('cleaned_data/target_val.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.44138\n",
      "[1]\tvalidation_0-logloss:0.30257\n",
      "[2]\tvalidation_0-logloss:0.21370\n",
      "[3]\tvalidation_0-logloss:0.15470\n",
      "[4]\tvalidation_0-logloss:0.11385\n",
      "[5]\tvalidation_0-logloss:0.08326\n",
      "[6]\tvalidation_0-logloss:0.06123\n",
      "[7]\tvalidation_0-logloss:0.04510\n",
      "[8]\tvalidation_0-logloss:0.03345\n",
      "[9]\tvalidation_0-logloss:0.02512\n",
      "[10]\tvalidation_0-logloss:0.01883\n",
      "[11]\tvalidation_0-logloss:0.01426\n",
      "[12]\tvalidation_0-logloss:0.01096\n",
      "[13]\tvalidation_0-logloss:0.00855\n",
      "[14]\tvalidation_0-logloss:0.00659\n",
      "[15]\tvalidation_0-logloss:0.00519\n",
      "[16]\tvalidation_0-logloss:0.00416\n",
      "[17]\tvalidation_0-logloss:0.00330\n",
      "[18]\tvalidation_0-logloss:0.00269\n",
      "[19]\tvalidation_0-logloss:0.00210\n",
      "[20]\tvalidation_0-logloss:0.00172\n",
      "[21]\tvalidation_0-logloss:0.00143\n",
      "[22]\tvalidation_0-logloss:0.00112\n",
      "[23]\tvalidation_0-logloss:0.00094\n",
      "[24]\tvalidation_0-logloss:0.00076\n",
      "[25]\tvalidation_0-logloss:0.00064\n",
      "[26]\tvalidation_0-logloss:0.00053\n",
      "[27]\tvalidation_0-logloss:0.00045\n",
      "[28]\tvalidation_0-logloss:0.00038\n",
      "[29]\tvalidation_0-logloss:0.00033\n",
      "[30]\tvalidation_0-logloss:0.00027\n",
      "[31]\tvalidation_0-logloss:0.00024\n",
      "[32]\tvalidation_0-logloss:0.00021\n",
      "[33]\tvalidation_0-logloss:0.00018\n",
      "[34]\tvalidation_0-logloss:0.00016\n",
      "[35]\tvalidation_0-logloss:0.00015\n",
      "[36]\tvalidation_0-logloss:0.00013\n",
      "[37]\tvalidation_0-logloss:0.00012\n",
      "[38]\tvalidation_0-logloss:0.00011\n",
      "[39]\tvalidation_0-logloss:0.00010\n",
      "[40]\tvalidation_0-logloss:0.00009\n",
      "[41]\tvalidation_0-logloss:0.00008\n",
      "[42]\tvalidation_0-logloss:0.00008\n",
      "[43]\tvalidation_0-logloss:0.00007\n",
      "[44]\tvalidation_0-logloss:0.00007\n",
      "[45]\tvalidation_0-logloss:0.00006\n",
      "[46]\tvalidation_0-logloss:0.00006\n",
      "[47]\tvalidation_0-logloss:0.00005\n",
      "[48]\tvalidation_0-logloss:0.00005\n",
      "[49]\tvalidation_0-logloss:0.00005\n",
      "[50]\tvalidation_0-logloss:0.00005\n",
      "[51]\tvalidation_0-logloss:0.00004\n",
      "[52]\tvalidation_0-logloss:0.00004\n",
      "[53]\tvalidation_0-logloss:0.00004\n",
      "[54]\tvalidation_0-logloss:0.00004\n",
      "[55]\tvalidation_0-logloss:0.00004\n",
      "[56]\tvalidation_0-logloss:0.00004\n",
      "[57]\tvalidation_0-logloss:0.00003\n",
      "[58]\tvalidation_0-logloss:0.00003\n",
      "[59]\tvalidation_0-logloss:0.00003\n",
      "[60]\tvalidation_0-logloss:0.00003\n",
      "[61]\tvalidation_0-logloss:0.00003\n",
      "[62]\tvalidation_0-logloss:0.00003\n",
      "[63]\tvalidation_0-logloss:0.00003\n",
      "[64]\tvalidation_0-logloss:0.00003\n",
      "[65]\tvalidation_0-logloss:0.00003\n",
      "[66]\tvalidation_0-logloss:0.00003\n",
      "[67]\tvalidation_0-logloss:0.00003\n",
      "[68]\tvalidation_0-logloss:0.00003\n",
      "[69]\tvalidation_0-logloss:0.00003\n",
      "[70]\tvalidation_0-logloss:0.00003\n",
      "[71]\tvalidation_0-logloss:0.00003\n",
      "[72]\tvalidation_0-logloss:0.00003\n",
      "[73]\tvalidation_0-logloss:0.00003\n",
      "[74]\tvalidation_0-logloss:0.00003\n",
      "[75]\tvalidation_0-logloss:0.00003\n",
      "[76]\tvalidation_0-logloss:0.00003\n",
      "[77]\tvalidation_0-logloss:0.00003\n",
      "[78]\tvalidation_0-logloss:0.00003\n",
      "[79]\tvalidation_0-logloss:0.00003\n",
      "[80]\tvalidation_0-logloss:0.00003\n",
      "[81]\tvalidation_0-logloss:0.00003\n",
      "[82]\tvalidation_0-logloss:0.00003\n",
      "[83]\tvalidation_0-logloss:0.00003\n",
      "[84]\tvalidation_0-logloss:0.00003\n",
      "[85]\tvalidation_0-logloss:0.00003\n",
      "[86]\tvalidation_0-logloss:0.00003\n",
      "[87]\tvalidation_0-logloss:0.00003\n",
      "[88]\tvalidation_0-logloss:0.00003\n",
      "[89]\tvalidation_0-logloss:0.00003\n",
      "[90]\tvalidation_0-logloss:0.00003\n",
      "[91]\tvalidation_0-logloss:0.00003\n",
      "[92]\tvalidation_0-logloss:0.00003\n",
      "[93]\tvalidation_0-logloss:0.00003\n",
      "[94]\tvalidation_0-logloss:0.00003\n",
      "[95]\tvalidation_0-logloss:0.00003\n",
      "[96]\tvalidation_0-logloss:0.00003\n",
      "[97]\tvalidation_0-logloss:0.00003\n",
      "[98]\tvalidation_0-logloss:0.00003\n",
      "[99]\tvalidation_0-logloss:0.00003\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the model\n",
    "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define evaluation set (using validation set here)\n",
    "eval_set = [(features_val, target_val)]\n",
    "\n",
    "# Fit the model and print logs\n",
    "xgb.fit(features_train, target_train, eval_set=eval_set, early_stopping_rounds=30, verbose=True)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_xgb = xgb.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 ... 0 0 0]\n",
      "Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    194329\n",
      "         1.0       1.00      1.00      1.00     54704\n",
      "\n",
      "    accuracy                           1.00    249033\n",
      "   macro avg       1.00      1.00      1.00    249033\n",
      "weighted avg       1.00      1.00      1.00    249033\n",
      "\n",
      "Confusion Matrix:\n",
      " [[194329      0]\n",
      " [     1  54703]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", predictions_xgb)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(target_test, predictions_xgb)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(target_test, predictions_xgb)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_mat = confusion_matrix(target_test, predictions_xgb)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_rf = rf.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to PyTorch tensors\n",
    "data['hops'] = data['hops'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['path_failure'] = data['path_failure'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1208988/3622384753.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data['path_amount'] = data['path_amount'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def str_to_tensor(s):\n",
    "    # Remove the 'tensor' part from the string\n",
    "    s = s.replace('tensor(', '').replace(')', '')\n",
    "    # Convert the string to a float\n",
    "    s = float(s)\n",
    "    # Convert the float to a PyTorch tensor\n",
    "    return torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "data['path_amount'] = data['path_amount'].apply(str_to_tensor)\n",
    "data['path_amount'] = data['path_amount'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1208988/796595839.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data['duration_seconds'] = data['duration_seconds'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "data['duration_seconds'] = data['duration_seconds'].apply(str_to_tensor)\n",
    "data['duration_seconds'] = data['duration_seconds'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(0.), tensor(2.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(0.), tensor(2.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops path_failure  \\\n",
       "0  [[tensor(0.), tensor(2.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "1  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "2  [[tensor(0.), tensor(2.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "3  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "4  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "\n",
       "         path_amount  \n",
       "0  tensor(50000000.)  \n",
       "1  tensor(50000000.)  \n",
       "2  tensor(50000000.)  \n",
       "3  tensor(50000000.)  \n",
       "4  tensor(50000000.)  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['duration_seconds'], axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('026ec3e3438308519a75ca4496822a6c1e229174fbcaadeeb174704c377112c331', 0),\n",
       " ('03aefa43fbb4009b21a4129d05953974b7dbabbbfb511921410080860fca8ee1f0', 1),\n",
       " ('027100442c3b79f606f80f322d98d499eefcb060599efc5d4ecb00209c2cb54190', 2),\n",
       " ('02a98e8c590a1b5602049d6b21d8f4c8861970aa310762f42eae1b2be88372e924', 3),\n",
       " ('033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025', 4)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pubkey_map = pd.read_pickle('pubkey_dict.pkl')\n",
    "list(pubkey_map.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8.747901429065974e+17, 0),\n",
       " (8.801975409563402e+17, 1),\n",
       " (8.88120621832536e+17, 2),\n",
       " (8.800612016854139e+17, 3),\n",
       " (8.69064985909592e+17, 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scid_map = pd.read_pickle('scid_dict.pkl')\n",
    "list(scid_map.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "features = data[['hops', 'path_amount']]\n",
    "target = data['path_failure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "features = data[['hops', 'path_amount']]\n",
    "target = data['path_failure']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Split data into train and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "features_train, features_val, target_train, target_val = train_test_split(features_train, target_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new directory to save the data\n",
    "os.makedirs('cleaned_data', exist_ok=True)\n",
    "\n",
    "# Save the data\n",
    "pickle.dump(features_train, open('cleaned_data/features_train.pkl', 'wb'))\n",
    "pickle.dump(target_train, open('cleaned_data/target_train.pkl', 'wb'))\n",
    "pickle.dump(features_test, open('cleaned_data/features_test.pkl', 'wb'))\n",
    "pickle.dump(target_test, open('cleaned_data/target_test.pkl', 'wb'))\n",
    "pickle.dump(features_val, open('cleaned_data/features_val.pkl', 'wb'))\n",
    "pickle.dump(target_val, open('cleaned_data/target_val.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hops</th>\n",
       "      <th>path_failure</th>\n",
       "      <th>path_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(0.), tensor(2.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(0.), tensor(2.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(0.), tensor(3.), tensor(0.), tensor(0...</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(50000000.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hops path_failure  \\\n",
       "0  [[tensor(0.), tensor(2.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "1  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "2  [[tensor(0.), tensor(2.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "3  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "4  [[tensor(0.), tensor(3.), tensor(0.), tensor(0...   tensor(1.)   \n",
       "\n",
       "         path_amount  \n",
       "0  tensor(50000000.)  \n",
       "1  tensor(50000000.)  \n",
       "2  tensor(50000000.)  \n",
       "3  tensor(50000000.)  \n",
       "4  tensor(50000000.)  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/paperspace/ml-final-project/new_models.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/ml-final-project/new_models.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/ml-final-project/new_models.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/ml-final-project/new_models.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m rf\u001b[39m.\u001b[39;49mfit(features_train, target_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/ml-final-project/new_models.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/ml-final-project/new_models.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m predictions_rf \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mpredict(features_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    332\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    333\u001b[0m )\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_rf = rf.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the model\n",
    "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Fit the model\n",
    "xgb.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_xgb = xgb.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the model\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ada.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_ada = ada.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LSTMNet(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim).to(self.device),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim).to(self.device))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
